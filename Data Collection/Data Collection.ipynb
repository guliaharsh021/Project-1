{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18c36c77-efe1-4128-8f54-df8d0aad9521",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "\n",
    "## Data Sources\n",
    "- Real estate website - [MagicBricks](https://www.magicbricks.com)\n",
    "\n",
    "## Data Types\n",
    "- Property Name\n",
    "- Properly Title\n",
    "- Property Type (Flat)\n",
    "- Property Size (Carpet Area)\n",
    "- Furnshing \n",
    "- BHK\n",
    "- City / Locality\n",
    "- Price\n",
    "- Price (SQFT)\n",
    "\n",
    "## Method\n",
    "- Web Scraping (BeautifulSoup and Selenium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e54a5239-93dc-44ca-b88f-5f5bc821e383",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guliaharsh021/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data frames were missed.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the possible values for bedrooms, property types, and cities\n",
    "cityname_values = ['New-Delhi', 'South-area-New-Delhi', 'East-area-New-Delhi', 'West-area-New-Delhi', 'Central-area-New-Delhi', 'North-area-New-Delhi']\n",
    "bedroom_values = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "proptype_values = [\"Multistorey-Apartment,Builder-Floor-Apartment,Penthouse,Studio-Apartment\"]\n",
    "\n",
    "# Define budget ranges as tuples\n",
    "budget_values = [\n",
    "    (0, 25), (25, 50), (50, 75), (75, 100), (100, 125), (125, 150), (150, 175), (175, 200), (200, 225), \n",
    "    (225, 250), (250, 275), (275, 300), (300, 350), (350, 400), (400, 450), (450, 700), \n",
    "    (700, 800), (800, 900), (900, 1000), (1000, 100000)\n",
    "]\n",
    "\n",
    "# Base URL with placeholders for bedroom, property type, city name, and budget values\n",
    "base_url = \"https://www.magicbricks.com/property-for-sale/residential-real-estate?bedroom={}&proptype={}&cityName={}&BudgetMin={}-Lacs&BudgetMax={}-Lacs\"\n",
    "\n",
    "# Container to hold all data frames\n",
    "data_frames = []\n",
    "\n",
    "# Set up the Safari driver\n",
    "driver = webdriver.Safari()\n",
    "\n",
    "# Dictionary to hold HTML content for each BHK value\n",
    "html_content_dict = {bedroom: \"\" for bedroom in bedroom_values}\n",
    "\n",
    "# List to hold missed data frame information\n",
    "missed_data_frames = []\n",
    "\n",
    "# Generate the list of URLs and iterate over them\n",
    "for cityname in cityname_values:\n",
    "    for bedroom in bedroom_values:\n",
    "        bhk_value = bedroom\n",
    "        for proptype in proptype_values:\n",
    "            for min_budget, max_budget in budget_values:\n",
    "                url = base_url.format(bhk_value, proptype, cityname, min_budget, max_budget)\n",
    "                \n",
    "                try:\n",
    "                    # Open the website\n",
    "                    driver.get(url)\n",
    "\n",
    "                    # Scroll down to load more data\n",
    "                    SCROLL_PAUSE_TIME = 5\n",
    "\n",
    "                    # Get scroll height\n",
    "                    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "                    while True:\n",
    "                        # Scroll down to the bottom\n",
    "                        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "                        # Wait to load the page\n",
    "                        time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "                        # Calculate new scroll height and compare with last scroll height\n",
    "                        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                        if new_height == last_height:\n",
    "                            break\n",
    "                        last_height = new_height\n",
    "\n",
    "                    # Get the page source\n",
    "                    page_source = driver.page_source\n",
    "\n",
    "                    # Parse the HTML data using BeautifulSoup\n",
    "                    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "                    # Append the current HTML content to the dictionary entry for the current BHK value\n",
    "                    html_content_dict[bhk_value] += str(soup)\n",
    "\n",
    "                    # Define the lists to store the data\n",
    "                    property_name = []\n",
    "                    property_title = []\n",
    "                    property_type = []\n",
    "                    city_or_locality = []\n",
    "                    bhk = []\n",
    "                    property_size = []\n",
    "                    furnishing = []\n",
    "                    price_total = []\n",
    "                    price_per_sqft = []\n",
    "\n",
    "                    # Parse the HTML data using BeautifulSoup\n",
    "                    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "                    # Find all the property details\n",
    "                    property_details = soup.find_all('div', class_=\"mb-srp__card__container\")\n",
    "\n",
    "                    for i in property_details:\n",
    "                        try:\n",
    "                            property_name.append(i.find('a', class_=\"mb-srp__card__society--name\").text.strip())\n",
    "                        except:\n",
    "                            property_name.append(np.nan)\n",
    "\n",
    "                        try:\n",
    "                            property_title.append(i.find('h2', class_=\"mb-srp__card--title\").text.strip())\n",
    "                        except:\n",
    "                            property_title.append(np.nan)\n",
    "\n",
    "                        # Set the property type manually\n",
    "                        property_type.append(\"Multistorey-Apartment,Builder-Floor-Apartment,Penthouse,Studio-Apartment\")\n",
    "\n",
    "                        try:\n",
    "                            property_size.append(i.find_all('div', class_=\"mb-srp__card__summary--value\")[0].text.strip())\n",
    "                        except:\n",
    "                            property_size.append(np.nan)\n",
    "\n",
    "                        # Set the BHK value manually based on the bedroom value\n",
    "                        bhk.append(bhk_value)\n",
    "\n",
    "                        try:\n",
    "                            furnishing.append(i.find_all('div', class_=\"mb-srp__card__summary--value\")[4].text.strip())\n",
    "                        except:\n",
    "                            furnishing.append(np.nan)\n",
    "\n",
    "                    # Function to extract city or locality (last two parts)\n",
    "                    def extract_city_or_locality(title):\n",
    "                        parts = title.split(',')\n",
    "                        if len(parts) >= 2:\n",
    "                            return ','.join(parts[-2:]).strip()\n",
    "                        return np.nan\n",
    "                    \n",
    "                    city_or_locality = [extract_city_or_locality(title) for title in property_title]\n",
    "\n",
    "                    # Find all the property price details\n",
    "                    property_price_details = soup.find_all('div', class_=\"mb-srp__card__price\")\n",
    "\n",
    "                    for i in property_price_details:\n",
    "                        try:\n",
    "                            price_total.append(i.find('div', class_=\"mb-srp__card__price--amount\").text.strip())\n",
    "                        except:\n",
    "                            price_total.append(np.nan)\n",
    "                        \n",
    "                        try:\n",
    "                            price_per_sqft.append(i.find('div', class_=\"mb-srp__card__price--size\").text.strip())\n",
    "                        except:\n",
    "                            price_per_sqft.append(np.nan)\n",
    "\n",
    "                    # Creating a DataFrame for the current iteration\n",
    "                    df = pd.DataFrame({\n",
    "                        'Property Name': property_name,\n",
    "                        'Property Title': property_title,\n",
    "                        'Property Type': property_type,\n",
    "                        'City/Locality': city_or_locality,\n",
    "                        'BHK': bhk,\n",
    "                        'Property Size': property_size,\n",
    "                        'Furnishing': furnishing,\n",
    "                        'Price Total': price_total,\n",
    "                        'Price per Sqft': price_per_sqft\n",
    "                    })\n",
    "\n",
    "                    # Append the DataFrame to the list\n",
    "                    data_frames.append(df)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing URL: {url}\")\n",
    "                    print(f\"Error: {e}\")\n",
    "                    missed_data_frames.append((cityname, bhk_value, proptype, min_budget, max_budget))\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n",
    "\n",
    "# Define the directory path\n",
    "directory = \"/Users/guliaharsh021/Downloads/DA Documents /Projects/Project 1/Data Collection/Flats Data/HTML\"\n",
    "\n",
    "# Save the HTML content to the specified path\n",
    "for bhk_value, html_content in html_content_dict.items():\n",
    "    file_path = os.path.join(directory, f'flats_{bhk_value}bhk_data.txt')\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(html_content)\n",
    "\n",
    "# Merge all data frames into one\n",
    "if data_frames:\n",
    "    final_df = pd.concat(data_frames, ignore_index=True)\n",
    "    # Save the final DataFrame to a CSV file\n",
    "    final_df.to_csv('/Users/guliaharsh021/Downloads/DA Documents /Projects/Project 1/Data Collection/Flats Data/flats_data.csv', index=False)\n",
    "else:\n",
    "    print(\"No data frames were created successfully.\")\n",
    "\n",
    "# Log missed data frames\n",
    "if missed_data_frames:\n",
    "    print(\"Missed the following data frames:\")\n",
    "    for item in missed_data_frames:\n",
    "        print(item)\n",
    "else:\n",
    "    print(\"No data frames were missed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc01fbe7-2cb5-4e1a-bef9-7d6b42110147",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
